{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ion.csv\")\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minority ist bad also bad = True\n",
    "X = df.copy()\n",
    "y = (X.pop(\"Class\") == \"bad\").to_numpy()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)# AUFPASSEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76      , 1.        , 0.        , ..., 0.37676   , 0.44053   ,\n",
       "        0.36391   ],\n",
       "       [0.18857143, 1.        , 0.        , ..., 0.389895  , 0.901775  ,\n",
       "        0.463355  ],\n",
       "       [0.36285714, 1.        , 0.        , ..., 0.446015  , 0.96572   ,\n",
       "        0.46556   ],\n",
       "       ...,\n",
       "       [0.79428571, 1.        , 0.        , ..., 0.720975  , 0.332585  ,\n",
       "        0.687325  ],\n",
       "       [0.57142857, 0.        , 0.        , ..., 0.        , 0.5       ,\n",
       "        0.5       ],\n",
       "       [0.37714286, 1.        , 0.        , ..., 0.        , 0.5       ,\n",
       "        0.5       ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def create_model(n_layers=3, n_units=64, dropout_rate=0.2, input_shape=[35], activation=\"relu\", output_units=1, output_activation=\"sigmoid\"):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        model.add(layers.Dense(n_units, activation=activation))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(layers.Dense(output_units, activation=output_activation))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6552 - val_loss: 0.6400\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.6531 - val_loss: 0.6334\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6437 - val_loss: 0.6285\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6349 - val_loss: 0.6236\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.6521 - val_loss: 0.6191\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.6361 - val_loss: 0.6146\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6298 - val_loss: 0.6100\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.6210 - val_loss: 0.6052\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6293 - val_loss: 0.6005\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.6091 - val_loss: 0.5961\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6435 - val_loss: 0.5925\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.6105 - val_loss: 0.5890\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.6201 - val_loss: 0.5854\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.5967 - val_loss: 0.5816\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.6036 - val_loss: 0.5777\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.5956 - val_loss: 0.5737\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.6019 - val_loss: 0.5697\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5838 - val_loss: 0.5656\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.5789 - val_loss: 0.5613\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.5823 - val_loss: 0.5561\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.5739 - val_loss: 0.5500\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.5892 - val_loss: 0.5439\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.5790 - val_loss: 0.5378\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.5807 - val_loss: 0.5315\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.5819 - val_loss: 0.5254\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.5585 - val_loss: 0.5187\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.5653 - val_loss: 0.5118\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5553 - val_loss: 0.5046\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.5505 - val_loss: 0.4975\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5266 - val_loss: 0.4900\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.5516 - val_loss: 0.4825\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.5316 - val_loss: 0.4748\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5302 - val_loss: 0.4669\n",
      "Epoch 34/10000\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.5201 - val_loss: 0.4593\n",
      "Epoch 35/10000\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.5390 - val_loss: 0.4517\n",
      "Epoch 36/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.5199 - val_loss: 0.4444\n",
      "Epoch 37/10000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.5121 - val_loss: 0.4365\n",
      "Epoch 38/10000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.4803 - val_loss: 0.4285\n",
      "Epoch 39/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4943 - val_loss: 0.4207\n",
      "Epoch 40/10000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.4898 - val_loss: 0.4139\n",
      "Epoch 41/10000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4855 - val_loss: 0.4077\n",
      "Epoch 42/10000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5060 - val_loss: 0.3996\n",
      "Epoch 43/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4885 - val_loss: 0.3899\n",
      "Epoch 44/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.4380 - val_loss: 0.3808\n",
      "Epoch 45/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4630 - val_loss: 0.3735\n",
      "Epoch 46/10000\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4501 - val_loss: 0.3672\n",
      "Epoch 47/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.4379 - val_loss: 0.3606\n",
      "Epoch 48/10000\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4504 - val_loss: 0.3515\n",
      "Epoch 49/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4147 - val_loss: 0.3431\n",
      "Epoch 50/10000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.3957 - val_loss: 0.3369\n",
      "Epoch 51/10000\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.4282 - val_loss: 0.3320\n",
      "Epoch 52/10000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.3780 - val_loss: 0.3278\n",
      "Epoch 53/10000\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4303 - val_loss: 0.3215\n",
      "Epoch 54/10000\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.3907 - val_loss: 0.3120\n",
      "Epoch 55/10000\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.3926 - val_loss: 0.3023\n",
      "Epoch 56/10000\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.3681 - val_loss: 0.2962\n",
      "Epoch 57/10000\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.3728 - val_loss: 0.2918\n",
      "Epoch 58/10000\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.3389 - val_loss: 0.2856\n",
      "Epoch 59/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.3766 - val_loss: 0.2753\n",
      "Epoch 60/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.3518 - val_loss: 0.2671\n",
      "Epoch 61/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.3266 - val_loss: 0.2615\n",
      "Epoch 62/10000\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.3093 - val_loss: 0.2574\n",
      "Epoch 63/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.3682 - val_loss: 0.2538\n",
      "Epoch 64/10000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.3183 - val_loss: 0.2461\n",
      "Epoch 65/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.3473 - val_loss: 0.2365\n",
      "Epoch 66/10000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.3130 - val_loss: 0.2297\n",
      "Epoch 67/10000\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.3078 - val_loss: 0.2263\n",
      "Epoch 68/10000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3071 - val_loss: 0.2216\n",
      "Epoch 69/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.2981 - val_loss: 0.2161\n",
      "Epoch 70/10000\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2953 - val_loss: 0.2118\n",
      "Epoch 71/10000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.3380 - val_loss: 0.2101\n",
      "Epoch 72/10000\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.2900 - val_loss: 0.2079\n",
      "Epoch 73/10000\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2970 - val_loss: 0.2020\n",
      "Epoch 74/10000\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 0.2578 - val_loss: 0.1954\n",
      "Epoch 75/10000\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.2574 - val_loss: 0.1905\n",
      "Epoch 76/10000\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.2842 - val_loss: 0.1884\n",
      "Epoch 77/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2885 - val_loss: 0.1847\n",
      "Epoch 78/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2810 - val_loss: 0.1808\n",
      "Epoch 79/10000\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.2569 - val_loss: 0.1763\n",
      "Epoch 80/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.2526 - val_loss: 0.1742\n",
      "Epoch 81/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2664 - val_loss: 0.1748\n",
      "Epoch 82/10000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2286 - val_loss: 0.1728\n",
      "Epoch 83/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.2573 - val_loss: 0.1695\n",
      "Epoch 84/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1971 - val_loss: 0.1659\n",
      "Epoch 85/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2803 - val_loss: 0.1636\n",
      "Epoch 86/10000\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2385 - val_loss: 0.1625\n",
      "Epoch 87/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2003 - val_loss: 0.1615\n",
      "Epoch 88/10000\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2210 - val_loss: 0.1608\n",
      "Epoch 89/10000\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.2293 - val_loss: 0.1600\n",
      "Epoch 90/10000\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2399 - val_loss: 0.1593\n",
      "Epoch 91/10000\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.2387 - val_loss: 0.1585\n",
      "Epoch 92/10000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.2459 - val_loss: 0.1573\n",
      "Epoch 93/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2261 - val_loss: 0.1565\n",
      "Epoch 94/10000\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.1814 - val_loss: 0.1556\n",
      "Epoch 95/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.2278 - val_loss: 0.1545\n",
      "Epoch 96/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1833 - val_loss: 0.1520\n",
      "Epoch 97/10000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1909 - val_loss: 0.1487\n",
      "Epoch 98/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1645 - val_loss: 0.1465\n",
      "Epoch 99/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1877 - val_loss: 0.1456\n",
      "Epoch 100/10000\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1677 - val_loss: 0.1443\n",
      "Epoch 101/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1965 - val_loss: 0.1430\n",
      "Epoch 102/10000\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.1872 - val_loss: 0.1416\n",
      "Epoch 103/10000\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.2102 - val_loss: 0.1402\n",
      "Epoch 104/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1948 - val_loss: 0.1394\n",
      "Epoch 105/10000\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.1731 - val_loss: 0.1384\n",
      "Epoch 106/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1860 - val_loss: 0.1367\n",
      "Epoch 107/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1773 - val_loss: 0.1349\n",
      "Epoch 108/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1861 - val_loss: 0.1341\n",
      "Epoch 109/10000\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.1999 - val_loss: 0.1332\n",
      "Epoch 110/10000\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1527 - val_loss: 0.1319\n",
      "Epoch 111/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1766 - val_loss: 0.1298\n",
      "Epoch 112/10000\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.1619 - val_loss: 0.1268\n",
      "Epoch 113/10000\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.1738 - val_loss: 0.1246\n",
      "Epoch 114/10000\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.2026 - val_loss: 0.1241\n",
      "Epoch 115/10000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1684 - val_loss: 0.1238\n",
      "Epoch 116/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1616 - val_loss: 0.1247\n",
      "Epoch 117/10000\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.1456 - val_loss: 0.1271\n",
      "Epoch 118/10000\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.1464 - val_loss: 0.1288\n",
      "Epoch 119/10000\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1386 - val_loss: 0.1292\n",
      "Epoch 120/10000\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.1712 - val_loss: 0.1285\n",
      "Epoch 121/10000\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.1523 - val_loss: 0.1286\n",
      "Epoch 122/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1513 - val_loss: 0.1316\n",
      "Epoch 123/10000\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.1404 - val_loss: 0.1315\n",
      "Epoch 124/10000\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.1531 - val_loss: 0.1309\n",
      "Epoch 125/10000\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.1480 - val_loss: 0.1295\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10000, batch_size=256, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 45ms/step\n",
      "0.9473684210526315\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_valid) >= 0.5 \n",
    "print(precision_score(y_valid, predictions))\n",
    "print(recall_score(y_valid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266    False\n",
       "66     False\n",
       "127    False\n",
       "106     True\n",
       "48     False\n",
       "       ...  \n",
       "52     False\n",
       "259    False\n",
       "278    False\n",
       "200     True\n",
       "132     True\n",
       "Name: Class, Length: 224, dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/path/to/pytorch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/User/Desktop/guido/ml-projects/tiktok-user-profiles/main.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/User/Desktop/guido/ml-projects/tiktok-user-profiles/main.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/User/Desktop/guido/ml-projects/tiktok-user-profiles/main.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/User/Desktop/guido/ml-projects/tiktok-user-profiles/main.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader, TensorDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_layers=3, n_units=64, dropout_rate=0.2, input_shape=35, activation=nn.ReLU(), output_units=1, output_activation=nn.Sigmoid()):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(nn.Linear(input_shape, n_units))\n",
    "            self.layers.append(activation)\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "            input_shape = n_units\n",
    "            \n",
    "        self.layers.append(nn.Linear(n_units, output_units))\n",
    "        self.layers.append(output_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# Assuming you have input data `X_train`, `y_train`, `X_valid`, and `y_valid`\n",
    "train_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "valid_data = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256)\n",
    "valid_loader = DataLoader(valid_data, batch_size=256)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:  # Check validation loss every 10 epochs\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(criterion(model(inputs).squeeze(), targets) for inputs, targets in valid_loader)\n",
    "        print(f'Epoch {epoch}, Validation loss: {valid_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
